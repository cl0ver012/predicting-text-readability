{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from utils.evaluation import print_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"../features/weebit_train_with_features.csv\", index_col=0)\n",
    "X_test = pd.read_csv(\"../features/weebit_test_with_features.csv\", index_col=0)\n",
    "\n",
    "# get Y\n",
    "y_train = X_train[\"Level\"]\n",
    "y_test = X_test[\"Level\"]\n",
    "\n",
    "# remove Y and Text columns \n",
    "X_train.drop(columns=['Text', 'Level'], inplace=True)\n",
    "X_test.drop(columns=['Text', 'Level'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.random_forest import RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Spearman's correlation coef: 0.7615175332600173\n",
      "================================================\n",
      "-----------\n",
      "R^2 = 0.5294429150491202\n",
      "-----------\n",
      "Confusion matrix:\n",
      "[[ 80  28   7   5   2]\n",
      " [ 17 107  22   5   7]\n",
      " [  1  25 113   4  17]\n",
      " [  6   9   7  82  24]\n",
      " [  1   9  17  22 111]]\n",
      "-----------\n",
      "Accuracy: 0.6771978021978022\n",
      "F1(micro)0.6771978021978022\n",
      "F1(macro)0.6786515124676925\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_test, y_pred, classification=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.xgboost import XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = XGBoost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost.fit(X_train, y_train)\n",
    "y_pred = xgboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Spearman's correlation coef: 0.7890308371249726\n",
      "================================================\n",
      "-----------\n",
      "R^2 = 0.5811758475813705\n",
      "-----------\n",
      "Confusion matrix:\n",
      "[[ 80  31   5   4   2]\n",
      " [ 17 112  18   5   6]\n",
      " [  3  27 114   3  13]\n",
      " [  5   7   6  82  28]\n",
      " [  2   7  10  27 114]]\n",
      "-----------\n",
      "Accuracy: 0.6895604395604396\n",
      "F1(micro)0.6895604395604396\n",
      "F1(macro)0.6893224224903831\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_test, y_pred, classification=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform hyperparameter optimization to find best C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'linear'\n",
    "Cs = [0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0, 5.0, 10.0, 50.0]\n",
    "bestC = find_best_C_for_SVC(X_train, y_train, kernel, Cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC(C=10.0, max_iter=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rinjac/anaconda3/envs/deep_nlp/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=10.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=20000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Spearman's correlation coef: 0.7566728905246934\n",
      "================================================\n",
      "-----------\n",
      "R^2 = 0.4996787620853598\n",
      "-----------\n",
      "Confusion matrix:\n",
      "[[ 78  32   4   6   2]\n",
      " [ 26 102  18   8   4]\n",
      " [  6  28  95   6  25]\n",
      " [  8   7   8  72  33]\n",
      " [  4   3  10  30 113]]\n",
      "-----------\n",
      "Accuracy: 0.6318681318681318\n",
      "F1(micro)0.6318681318681318\n",
      "F1(macro)0.6296434044975326\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "y_pred = svc.predict(X_test)\n",
    "    \n",
    "print_metrics(y_test, y_pred, classification=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from models.multilayer_perceptron import MultilayerPerceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rinjac/anaconda3/envs/deep_nlp/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/rinjac/anaconda3/envs/deep_nlp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "mlp = MultilayerPerceptron(input_dim=X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rinjac/anaconda3/envs/deep_nlp/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2328 samples, validate on 583 samples\n",
      "Epoch 1/150\n",
      "2328/2328 [==============================] - 1s 521us/step - loss: 2.0777 - acc: 0.2169 - val_loss: 1.5722 - val_acc: 0.2333\n",
      "Epoch 2/150\n",
      "2328/2328 [==============================] - 0s 67us/step - loss: 1.6864 - acc: 0.2225 - val_loss: 1.5637 - val_acc: 0.2727\n",
      "Epoch 3/150\n",
      "2328/2328 [==============================] - 0s 65us/step - loss: 1.5886 - acc: 0.2706 - val_loss: 1.5495 - val_acc: 0.3156\n",
      "Epoch 4/150\n",
      "2328/2328 [==============================] - 0s 83us/step - loss: 1.5687 - acc: 0.2685 - val_loss: 1.5194 - val_acc: 0.3310\n",
      "Epoch 5/150\n",
      "2328/2328 [==============================] - 0s 66us/step - loss: 1.5479 - acc: 0.2930 - val_loss: 1.4667 - val_acc: 0.3602\n",
      "Epoch 6/150\n",
      "2328/2328 [==============================] - 0s 69us/step - loss: 1.5185 - acc: 0.3058 - val_loss: 1.4475 - val_acc: 0.3654\n",
      "Epoch 7/150\n",
      "2328/2328 [==============================] - 0s 70us/step - loss: 1.4984 - acc: 0.3157 - val_loss: 1.4292 - val_acc: 0.4048\n",
      "Epoch 8/150\n",
      "2328/2328 [==============================] - 0s 73us/step - loss: 1.4841 - acc: 0.3346 - val_loss: 1.4055 - val_acc: 0.4408\n",
      "Epoch 9/150\n",
      "2328/2328 [==============================] - 0s 68us/step - loss: 1.4533 - acc: 0.3535 - val_loss: 1.4120 - val_acc: 0.4494\n",
      "Epoch 10/150\n",
      "2328/2328 [==============================] - 0s 66us/step - loss: 1.4393 - acc: 0.3746 - val_loss: 1.3749 - val_acc: 0.4666\n",
      "Epoch 11/150\n",
      "2328/2328 [==============================] - 0s 77us/step - loss: 1.4315 - acc: 0.3673 - val_loss: 1.3752 - val_acc: 0.4288\n",
      "Epoch 12/150\n",
      "2328/2328 [==============================] - 0s 66us/step - loss: 1.4144 - acc: 0.3771 - val_loss: 1.3491 - val_acc: 0.4786\n",
      "Epoch 13/150\n",
      "2328/2328 [==============================] - 0s 70us/step - loss: 1.4013 - acc: 0.3973 - val_loss: 1.3404 - val_acc: 0.4906\n",
      "Epoch 14/150\n",
      "2328/2328 [==============================] - 0s 71us/step - loss: 1.3858 - acc: 0.4119 - val_loss: 1.3291 - val_acc: 0.4734\n",
      "Epoch 15/150\n",
      "2328/2328 [==============================] - 0s 66us/step - loss: 1.3699 - acc: 0.4180 - val_loss: 1.3031 - val_acc: 0.4940\n",
      "Epoch 16/150\n",
      "2328/2328 [==============================] - 0s 81us/step - loss: 1.3625 - acc: 0.4227 - val_loss: 1.2925 - val_acc: 0.5163\n",
      "Epoch 17/150\n",
      "2328/2328 [==============================] - 0s 75us/step - loss: 1.3469 - acc: 0.4274 - val_loss: 1.2721 - val_acc: 0.5111\n",
      "Epoch 18/150\n",
      "2328/2328 [==============================] - 0s 76us/step - loss: 1.3614 - acc: 0.4261 - val_loss: 1.2868 - val_acc: 0.5111\n",
      "Epoch 19/150\n",
      "2328/2328 [==============================] - 0s 99us/step - loss: 1.3367 - acc: 0.4394 - val_loss: 1.2639 - val_acc: 0.5249\n",
      "Epoch 20/150\n",
      "2328/2328 [==============================] - 0s 82us/step - loss: 1.3208 - acc: 0.4356 - val_loss: 1.2276 - val_acc: 0.5403\n",
      "Epoch 21/150\n",
      "2328/2328 [==============================] - 0s 67us/step - loss: 1.3041 - acc: 0.4497 - val_loss: 1.2246 - val_acc: 0.5317\n",
      "Epoch 22/150\n",
      "2328/2328 [==============================] - 0s 70us/step - loss: 1.2936 - acc: 0.4781 - val_loss: 1.2152 - val_acc: 0.5266\n",
      "Epoch 23/150\n",
      "2328/2328 [==============================] - 0s 73us/step - loss: 1.2911 - acc: 0.4618 - val_loss: 1.2004 - val_acc: 0.5403\n",
      "Epoch 24/150\n",
      "2328/2328 [==============================] - 0s 68us/step - loss: 1.2571 - acc: 0.4854 - val_loss: 1.1674 - val_acc: 0.5523\n",
      "Epoch 25/150\n",
      "2328/2328 [==============================] - 0s 76us/step - loss: 1.2599 - acc: 0.4875 - val_loss: 1.1747 - val_acc: 0.5403\n",
      "Epoch 26/150\n",
      "2328/2328 [==============================] - 0s 66us/step - loss: 1.2447 - acc: 0.4824 - val_loss: 1.1465 - val_acc: 0.5592\n",
      "Epoch 27/150\n",
      "2328/2328 [==============================] - 0s 71us/step - loss: 1.2405 - acc: 0.4974 - val_loss: 1.1252 - val_acc: 0.5540\n",
      "Epoch 28/150\n",
      "2328/2328 [==============================] - 0s 72us/step - loss: 1.2217 - acc: 0.4974 - val_loss: 1.1364 - val_acc: 0.5609\n",
      "Epoch 29/150\n",
      "2328/2328 [==============================] - 0s 72us/step - loss: 1.2068 - acc: 0.5009 - val_loss: 1.1211 - val_acc: 0.5660\n",
      "Epoch 30/150\n",
      "2328/2328 [==============================] - 0s 72us/step - loss: 1.1939 - acc: 0.5095 - val_loss: 1.1074 - val_acc: 0.5472\n",
      "Epoch 31/150\n",
      "2328/2328 [==============================] - 0s 67us/step - loss: 1.2044 - acc: 0.4983 - val_loss: 1.1042 - val_acc: 0.5695\n",
      "Epoch 32/150\n",
      "2328/2328 [==============================] - 0s 69us/step - loss: 1.1700 - acc: 0.5180 - val_loss: 1.0865 - val_acc: 0.5678\n",
      "Epoch 33/150\n",
      "2328/2328 [==============================] - 0s 74us/step - loss: 1.1786 - acc: 0.5210 - val_loss: 1.0775 - val_acc: 0.5763\n",
      "Epoch 34/150\n",
      "2328/2328 [==============================] - 0s 73us/step - loss: 1.1789 - acc: 0.5155 - val_loss: 1.0769 - val_acc: 0.5798\n",
      "Epoch 35/150\n",
      "2328/2328 [==============================] - 0s 83us/step - loss: 1.1797 - acc: 0.5155 - val_loss: 1.0963 - val_acc: 0.5780\n",
      "Epoch 36/150\n",
      "2328/2328 [==============================] - 0s 69us/step - loss: 1.1518 - acc: 0.5318 - val_loss: 1.0629 - val_acc: 0.5712\n",
      "Epoch 37/150\n",
      "2328/2328 [==============================] - 0s 75us/step - loss: 1.1546 - acc: 0.5344 - val_loss: 1.0641 - val_acc: 0.5832\n",
      "Epoch 38/150\n",
      "2328/2328 [==============================] - 0s 69us/step - loss: 1.1239 - acc: 0.5460 - val_loss: 1.0594 - val_acc: 0.5969\n",
      "Epoch 39/150\n",
      "2328/2328 [==============================] - 0s 76us/step - loss: 1.1512 - acc: 0.5378 - val_loss: 1.0833 - val_acc: 0.5780\n",
      "Epoch 40/150\n",
      "2328/2328 [==============================] - 0s 84us/step - loss: 1.1276 - acc: 0.5417 - val_loss: 1.0426 - val_acc: 0.6003\n",
      "Epoch 41/150\n",
      "2328/2328 [==============================] - 0s 74us/step - loss: 1.1063 - acc: 0.5494 - val_loss: 1.0308 - val_acc: 0.6141\n",
      "Epoch 42/150\n",
      "2328/2328 [==============================] - 0s 70us/step - loss: 1.1141 - acc: 0.5344 - val_loss: 1.0904 - val_acc: 0.5592\n",
      "Epoch 43/150\n",
      "2328/2328 [==============================] - 0s 66us/step - loss: 1.1241 - acc: 0.5412 - val_loss: 1.0301 - val_acc: 0.5952\n",
      "Epoch 44/150\n",
      "2328/2328 [==============================] - 0s 92us/step - loss: 1.1040 - acc: 0.5494 - val_loss: 1.0368 - val_acc: 0.5969\n",
      "Epoch 45/150\n",
      "2328/2328 [==============================] - 0s 71us/step - loss: 1.0938 - acc: 0.5550 - val_loss: 1.0196 - val_acc: 0.6141\n",
      "Epoch 46/150\n",
      "2328/2328 [==============================] - 0s 74us/step - loss: 1.1199 - acc: 0.5511 - val_loss: 1.0202 - val_acc: 0.6072\n",
      "Epoch 47/150\n",
      "2328/2328 [==============================] - 0s 72us/step - loss: 1.1144 - acc: 0.5455 - val_loss: 1.0299 - val_acc: 0.6192\n",
      "Epoch 48/150\n",
      "2328/2328 [==============================] - 0s 70us/step - loss: 1.0838 - acc: 0.5528 - val_loss: 1.0136 - val_acc: 0.6055\n",
      "Epoch 49/150\n",
      "2328/2328 [==============================] - 0s 69us/step - loss: 1.0919 - acc: 0.5533 - val_loss: 1.0219 - val_acc: 0.5935\n",
      "Epoch 50/150\n",
      "2328/2328 [==============================] - 0s 75us/step - loss: 1.1075 - acc: 0.5597 - val_loss: 1.0168 - val_acc: 0.6158\n",
      "Epoch 51/150\n",
      "2328/2328 [==============================] - 0s 84us/step - loss: 1.0924 - acc: 0.5464 - val_loss: 1.0083 - val_acc: 0.6089\n",
      "Epoch 52/150\n",
      "2328/2328 [==============================] - 0s 81us/step - loss: 1.0759 - acc: 0.5662 - val_loss: 1.0189 - val_acc: 0.6192\n",
      "Epoch 53/150\n",
      "2328/2328 [==============================] - 0s 69us/step - loss: 1.0829 - acc: 0.5563 - val_loss: 1.0081 - val_acc: 0.6003\n",
      "Epoch 54/150\n",
      "2328/2328 [==============================] - 0s 83us/step - loss: 1.0808 - acc: 0.5614 - val_loss: 1.0008 - val_acc: 0.6209\n",
      "Epoch 55/150\n",
      "2328/2328 [==============================] - 0s 71us/step - loss: 1.0642 - acc: 0.5554 - val_loss: 1.0148 - val_acc: 0.6106\n",
      "Epoch 56/150\n",
      "2328/2328 [==============================] - 0s 75us/step - loss: 1.0654 - acc: 0.5571 - val_loss: 1.0086 - val_acc: 0.6089\n",
      "Epoch 57/150\n",
      "2328/2328 [==============================] - 0s 67us/step - loss: 1.0584 - acc: 0.5700 - val_loss: 0.9983 - val_acc: 0.6209\n",
      "Epoch 58/150\n",
      "2328/2328 [==============================] - 0s 74us/step - loss: 1.0760 - acc: 0.5640 - val_loss: 0.9852 - val_acc: 0.6141\n",
      "Epoch 59/150\n",
      "2328/2328 [==============================] - 0s 84us/step - loss: 1.0375 - acc: 0.5795 - val_loss: 0.9828 - val_acc: 0.6346\n",
      "Epoch 60/150\n",
      "2328/2328 [==============================] - 0s 77us/step - loss: 1.0490 - acc: 0.5700 - val_loss: 0.9887 - val_acc: 0.6175\n",
      "Epoch 61/150\n",
      "2328/2328 [==============================] - 0s 86us/step - loss: 1.0549 - acc: 0.5773 - val_loss: 0.9833 - val_acc: 0.6226\n",
      "Epoch 62/150\n",
      "2328/2328 [==============================] - 0s 88us/step - loss: 1.0490 - acc: 0.5808 - val_loss: 0.9787 - val_acc: 0.6295\n",
      "Epoch 63/150\n",
      "2328/2328 [==============================] - 0s 90us/step - loss: 1.0605 - acc: 0.5640 - val_loss: 0.9937 - val_acc: 0.6226\n",
      "Epoch 64/150\n",
      "2328/2328 [==============================] - 0s 78us/step - loss: 1.0492 - acc: 0.5825 - val_loss: 1.0032 - val_acc: 0.6312\n",
      "Epoch 65/150\n",
      "2328/2328 [==============================] - 0s 73us/step - loss: 1.0416 - acc: 0.5666 - val_loss: 0.9863 - val_acc: 0.6226\n",
      "Epoch 66/150\n",
      "2328/2328 [==============================] - 0s 74us/step - loss: 1.0343 - acc: 0.5868 - val_loss: 0.9893 - val_acc: 0.6261\n",
      "Epoch 67/150\n",
      "2328/2328 [==============================] - 0s 71us/step - loss: 1.0316 - acc: 0.5820 - val_loss: 0.9666 - val_acc: 0.6261\n",
      "Epoch 68/150\n",
      "2328/2328 [==============================] - 0s 70us/step - loss: 1.0291 - acc: 0.5859 - val_loss: 0.9681 - val_acc: 0.6312\n",
      "Epoch 69/150\n",
      "2328/2328 [==============================] - 0s 73us/step - loss: 1.0223 - acc: 0.5919 - val_loss: 0.9691 - val_acc: 0.6278\n",
      "Epoch 70/150\n",
      "2328/2328 [==============================] - 0s 73us/step - loss: 1.0231 - acc: 0.5855 - val_loss: 0.9699 - val_acc: 0.6261\n",
      "Epoch 71/150\n",
      "2328/2328 [==============================] - 0s 82us/step - loss: 1.0557 - acc: 0.5696 - val_loss: 0.9784 - val_acc: 0.6209\n",
      "Epoch 72/150\n",
      "2328/2328 [==============================] - 0s 86us/step - loss: 1.0275 - acc: 0.5825 - val_loss: 0.9738 - val_acc: 0.6381\n",
      "Epoch 73/150\n",
      "2328/2328 [==============================] - 0s 78us/step - loss: 1.0221 - acc: 0.5833 - val_loss: 0.9766 - val_acc: 0.6329\n",
      "Epoch 74/150\n",
      "2328/2328 [==============================] - 0s 72us/step - loss: 1.0130 - acc: 0.5911 - val_loss: 0.9770 - val_acc: 0.6226\n",
      "Epoch 75/150\n",
      "2328/2328 [==============================] - 0s 87us/step - loss: 1.0116 - acc: 0.5928 - val_loss: 0.9639 - val_acc: 0.6364\n",
      "Epoch 76/150\n",
      "2328/2328 [==============================] - 0s 84us/step - loss: 1.0060 - acc: 0.5958 - val_loss: 0.9534 - val_acc: 0.6329\n",
      "Epoch 77/150\n",
      "2328/2328 [==============================] - 0s 86us/step - loss: 1.0008 - acc: 0.5958 - val_loss: 0.9580 - val_acc: 0.6261\n",
      "Epoch 78/150\n",
      "2328/2328 [==============================] - 0s 89us/step - loss: 1.0118 - acc: 0.5855 - val_loss: 0.9903 - val_acc: 0.6141\n",
      "Epoch 79/150\n",
      "2328/2328 [==============================] - 0s 80us/step - loss: 1.0304 - acc: 0.5799 - val_loss: 0.9647 - val_acc: 0.6346\n",
      "Epoch 80/150\n",
      "2328/2328 [==============================] - 0s 72us/step - loss: 1.0036 - acc: 0.6065 - val_loss: 0.9507 - val_acc: 0.6381\n",
      "Epoch 81/150\n",
      "2328/2328 [==============================] - 0s 78us/step - loss: 1.0019 - acc: 0.5941 - val_loss: 0.9639 - val_acc: 0.6364\n",
      "Epoch 82/150\n",
      "2328/2328 [==============================] - 0s 86us/step - loss: 1.0283 - acc: 0.5803 - val_loss: 0.9963 - val_acc: 0.6226\n",
      "Epoch 83/150\n",
      "2328/2328 [==============================] - 0s 89us/step - loss: 1.0120 - acc: 0.5906 - val_loss: 0.9485 - val_acc: 0.6398\n",
      "Epoch 84/150\n",
      "2328/2328 [==============================] - 0s 74us/step - loss: 0.9949 - acc: 0.5975 - val_loss: 0.9525 - val_acc: 0.6381\n",
      "Epoch 85/150\n",
      "2328/2328 [==============================] - 0s 69us/step - loss: 0.9972 - acc: 0.6074 - val_loss: 0.9770 - val_acc: 0.6226\n",
      "Epoch 86/150\n",
      "2328/2328 [==============================] - 0s 74us/step - loss: 1.0028 - acc: 0.5816 - val_loss: 0.9466 - val_acc: 0.6449\n",
      "Epoch 87/150\n",
      "2328/2328 [==============================] - 0s 69us/step - loss: 0.9916 - acc: 0.6044 - val_loss: 0.9406 - val_acc: 0.6415\n",
      "Epoch 88/150\n",
      "2328/2328 [==============================] - 0s 75us/step - loss: 0.9898 - acc: 0.5924 - val_loss: 0.9583 - val_acc: 0.6501\n",
      "Epoch 89/150\n",
      "2328/2328 [==============================] - 0s 77us/step - loss: 1.0024 - acc: 0.6057 - val_loss: 0.9415 - val_acc: 0.6467\n",
      "Epoch 90/150\n",
      "2328/2328 [==============================] - 0s 87us/step - loss: 0.9881 - acc: 0.6035 - val_loss: 0.9469 - val_acc: 0.6398\n",
      "Epoch 91/150\n",
      "2328/2328 [==============================] - 0s 68us/step - loss: 0.9952 - acc: 0.5915 - val_loss: 0.9461 - val_acc: 0.6381\n",
      "Epoch 92/150\n",
      "2328/2328 [==============================] - 0s 78us/step - loss: 0.9843 - acc: 0.6065 - val_loss: 0.9579 - val_acc: 0.6398\n",
      "Epoch 93/150\n",
      "2328/2328 [==============================] - 0s 89us/step - loss: 0.9653 - acc: 0.6168 - val_loss: 0.9345 - val_acc: 0.6518\n",
      "Epoch 94/150\n",
      "2328/2328 [==============================] - 0s 76us/step - loss: 0.9852 - acc: 0.6048 - val_loss: 0.9337 - val_acc: 0.6518\n",
      "Epoch 95/150\n",
      "2328/2328 [==============================] - 0s 81us/step - loss: 0.9858 - acc: 0.5997 - val_loss: 0.9430 - val_acc: 0.6484\n",
      "Epoch 96/150\n",
      "2328/2328 [==============================] - 0s 91us/step - loss: 0.9733 - acc: 0.6125 - val_loss: 0.9380 - val_acc: 0.6329\n",
      "Epoch 97/150\n",
      "2328/2328 [==============================] - 0s 65us/step - loss: 0.9898 - acc: 0.6095 - val_loss: 0.9759 - val_acc: 0.6261\n",
      "Epoch 98/150\n",
      "2328/2328 [==============================] - 0s 89us/step - loss: 0.9747 - acc: 0.6061 - val_loss: 0.9483 - val_acc: 0.6432\n",
      "Epoch 99/150\n",
      "2328/2328 [==============================] - 0s 80us/step - loss: 0.9741 - acc: 0.6104 - val_loss: 0.9390 - val_acc: 0.6398\n",
      "Epoch 100/150\n",
      "2328/2328 [==============================] - 0s 76us/step - loss: 0.9712 - acc: 0.6048 - val_loss: 0.9444 - val_acc: 0.6398\n",
      "Epoch 101/150\n",
      "2328/2328 [==============================] - 0s 94us/step - loss: 0.9655 - acc: 0.6031 - val_loss: 0.9358 - val_acc: 0.6449\n",
      "Epoch 102/150\n",
      "2328/2328 [==============================] - 0s 75us/step - loss: 0.9630 - acc: 0.6052 - val_loss: 0.9250 - val_acc: 0.6621\n",
      "Epoch 103/150\n",
      "2328/2328 [==============================] - 0s 92us/step - loss: 0.9635 - acc: 0.6044 - val_loss: 0.9355 - val_acc: 0.6329\n",
      "Epoch 104/150\n",
      "2328/2328 [==============================] - 0s 72us/step - loss: 0.9673 - acc: 0.6177 - val_loss: 0.9366 - val_acc: 0.6432\n",
      "Epoch 105/150\n",
      "2328/2328 [==============================] - 0s 71us/step - loss: 0.9691 - acc: 0.6091 - val_loss: 0.9269 - val_acc: 0.6467\n",
      "Epoch 106/150\n",
      "2328/2328 [==============================] - 0s 81us/step - loss: 0.9628 - acc: 0.6130 - val_loss: 0.9282 - val_acc: 0.6381\n",
      "Epoch 107/150\n",
      "2328/2328 [==============================] - 0s 87us/step - loss: 0.9649 - acc: 0.6027 - val_loss: 0.9292 - val_acc: 0.6329\n",
      "Epoch 108/150\n",
      "2328/2328 [==============================] - 0s 96us/step - loss: 0.9739 - acc: 0.6194 - val_loss: 0.9341 - val_acc: 0.6449\n",
      "Epoch 109/150\n",
      "2328/2328 [==============================] - 0s 88us/step - loss: 0.9717 - acc: 0.6018 - val_loss: 0.9408 - val_acc: 0.6432\n",
      "Epoch 110/150\n",
      "2328/2328 [==============================] - 0s 86us/step - loss: 0.9638 - acc: 0.5992 - val_loss: 0.9418 - val_acc: 0.6398\n",
      "Epoch 111/150\n",
      "2328/2328 [==============================] - 0s 86us/step - loss: 0.9678 - acc: 0.6074 - val_loss: 0.9230 - val_acc: 0.6552\n",
      "Epoch 112/150\n",
      "2328/2328 [==============================] - 0s 81us/step - loss: 0.9565 - acc: 0.6035 - val_loss: 0.9607 - val_acc: 0.6329\n",
      "Epoch 113/150\n",
      "2328/2328 [==============================] - 0s 91us/step - loss: 0.9650 - acc: 0.6168 - val_loss: 0.9194 - val_acc: 0.6398\n",
      "Epoch 114/150\n",
      "2328/2328 [==============================] - 0s 80us/step - loss: 0.9553 - acc: 0.6134 - val_loss: 0.9175 - val_acc: 0.6467\n",
      "Epoch 115/150\n",
      "2328/2328 [==============================] - 0s 83us/step - loss: 0.9612 - acc: 0.6035 - val_loss: 0.9235 - val_acc: 0.6364\n",
      "Epoch 116/150\n",
      "2328/2328 [==============================] - 0s 92us/step - loss: 0.9445 - acc: 0.6164 - val_loss: 0.9286 - val_acc: 0.6501\n",
      "Epoch 117/150\n",
      "2328/2328 [==============================] - 0s 67us/step - loss: 0.9605 - acc: 0.6104 - val_loss: 0.9799 - val_acc: 0.6192\n",
      "Epoch 118/150\n",
      "2328/2328 [==============================] - 0s 79us/step - loss: 0.9653 - acc: 0.6117 - val_loss: 0.9285 - val_acc: 0.6415\n",
      "Epoch 119/150\n",
      "2328/2328 [==============================] - 0s 74us/step - loss: 0.9521 - acc: 0.6070 - val_loss: 0.9236 - val_acc: 0.6398\n",
      "Epoch 120/150\n",
      "2328/2328 [==============================] - 0s 80us/step - loss: 0.9434 - acc: 0.6263 - val_loss: 0.9135 - val_acc: 0.6587\n",
      "Epoch 121/150\n",
      "2328/2328 [==============================] - 0s 85us/step - loss: 0.9438 - acc: 0.6164 - val_loss: 0.9275 - val_acc: 0.6501\n",
      "Epoch 122/150\n",
      "2328/2328 [==============================] - 0s 102us/step - loss: 0.9479 - acc: 0.6190 - val_loss: 0.9273 - val_acc: 0.6398\n",
      "Epoch 123/150\n",
      "2328/2328 [==============================] - 0s 82us/step - loss: 0.9540 - acc: 0.6229 - val_loss: 0.9276 - val_acc: 0.6346\n",
      "Epoch 124/150\n",
      "2328/2328 [==============================] - 0s 71us/step - loss: 0.9489 - acc: 0.6271 - val_loss: 0.9128 - val_acc: 0.6484\n",
      "Epoch 125/150\n",
      "2328/2328 [==============================] - 0s 88us/step - loss: 0.9439 - acc: 0.6164 - val_loss: 0.9211 - val_acc: 0.6398\n",
      "Epoch 126/150\n",
      "2328/2328 [==============================] - 0s 82us/step - loss: 0.9402 - acc: 0.6306 - val_loss: 0.9289 - val_acc: 0.6484\n",
      "Epoch 127/150\n",
      "2328/2328 [==============================] - 0s 80us/step - loss: 0.9376 - acc: 0.6267 - val_loss: 0.9177 - val_acc: 0.6535\n",
      "Epoch 128/150\n",
      "2328/2328 [==============================] - 0s 107us/step - loss: 0.9464 - acc: 0.6203 - val_loss: 0.9508 - val_acc: 0.6141\n",
      "Epoch 129/150\n",
      "2328/2328 [==============================] - 0s 76us/step - loss: 0.9264 - acc: 0.6306 - val_loss: 0.9148 - val_acc: 0.6518\n",
      "Epoch 130/150\n",
      "2328/2328 [==============================] - 0s 85us/step - loss: 0.9277 - acc: 0.6259 - val_loss: 0.9265 - val_acc: 0.6415\n",
      "Epoch 131/150\n",
      "2328/2328 [==============================] - 0s 67us/step - loss: 0.9298 - acc: 0.6250 - val_loss: 0.9187 - val_acc: 0.6432\n",
      "Epoch 132/150\n",
      "2328/2328 [==============================] - 0s 82us/step - loss: 0.9276 - acc: 0.6250 - val_loss: 0.9131 - val_acc: 0.6381\n",
      "Epoch 133/150\n",
      "2328/2328 [==============================] - 0s 79us/step - loss: 0.9105 - acc: 0.6284 - val_loss: 0.9047 - val_acc: 0.6535\n",
      "Epoch 134/150\n",
      "2328/2328 [==============================] - 0s 63us/step - loss: 0.9319 - acc: 0.6271 - val_loss: 0.9320 - val_acc: 0.6415\n",
      "Epoch 135/150\n",
      "2328/2328 [==============================] - 0s 75us/step - loss: 0.9346 - acc: 0.6220 - val_loss: 0.9249 - val_acc: 0.6398\n",
      "Epoch 136/150\n",
      "2328/2328 [==============================] - 0s 78us/step - loss: 0.9310 - acc: 0.6250 - val_loss: 0.9183 - val_acc: 0.6501\n",
      "Epoch 137/150\n",
      "2328/2328 [==============================] - 0s 78us/step - loss: 0.9417 - acc: 0.6254 - val_loss: 0.9093 - val_acc: 0.6707\n",
      "Epoch 138/150\n",
      "2328/2328 [==============================] - 0s 82us/step - loss: 0.9128 - acc: 0.6306 - val_loss: 0.9184 - val_acc: 0.6552\n",
      "Epoch 139/150\n",
      "2328/2328 [==============================] - 0s 97us/step - loss: 0.9185 - acc: 0.6336 - val_loss: 0.9395 - val_acc: 0.6432\n",
      "Epoch 140/150\n",
      "2328/2328 [==============================] - 0s 87us/step - loss: 0.9140 - acc: 0.6340 - val_loss: 0.9115 - val_acc: 0.6415\n",
      "Epoch 141/150\n",
      "2328/2328 [==============================] - 0s 83us/step - loss: 0.9215 - acc: 0.6280 - val_loss: 0.9138 - val_acc: 0.6467\n",
      "Epoch 142/150\n",
      "2328/2328 [==============================] - 0s 74us/step - loss: 0.9360 - acc: 0.6224 - val_loss: 0.9132 - val_acc: 0.6552\n",
      "Epoch 143/150\n",
      "2328/2328 [==============================] - 0s 77us/step - loss: 0.9350 - acc: 0.6237 - val_loss: 0.9041 - val_acc: 0.6535\n",
      "Epoch 144/150\n",
      "2328/2328 [==============================] - 0s 82us/step - loss: 0.9245 - acc: 0.6284 - val_loss: 0.9068 - val_acc: 0.6621\n",
      "Epoch 145/150\n",
      "2328/2328 [==============================] - 0s 78us/step - loss: 0.9516 - acc: 0.6168 - val_loss: 0.9093 - val_acc: 0.6346\n",
      "Epoch 146/150\n",
      "2328/2328 [==============================] - 0s 81us/step - loss: 0.9183 - acc: 0.6302 - val_loss: 0.9137 - val_acc: 0.6484\n",
      "Epoch 147/150\n",
      "2328/2328 [==============================] - 0s 68us/step - loss: 0.9254 - acc: 0.6319 - val_loss: 0.9268 - val_acc: 0.6398\n",
      "Epoch 148/150\n",
      "2328/2328 [==============================] - 0s 90us/step - loss: 0.9232 - acc: 0.6237 - val_loss: 0.9022 - val_acc: 0.6587\n",
      "Epoch 149/150\n",
      "2328/2328 [==============================] - 0s 77us/step - loss: 0.9036 - acc: 0.6379 - val_loss: 0.9288 - val_acc: 0.6638\n",
      "Epoch 150/150\n",
      "2328/2328 [==============================] - 0s 82us/step - loss: 0.9255 - acc: 0.6375 - val_loss: 0.9054 - val_acc: 0.6569\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cat = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred_cat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Spearman's correlation coef: 0.7921287528133497\n",
      "================================================\n",
      "-----------\n",
      "R^2 = 0.593223242828607\n",
      "-----------\n",
      "Confusion matrix:\n",
      "[[ 72  40   4   5   1]\n",
      " [ 16 115  22   3   2]\n",
      " [  0  33 110   6  11]\n",
      " [  5   8   9  77  29]\n",
      " [  4   4  16  27 109]]\n",
      "-----------\n",
      "Accuracy: 0.6634615384615384\n",
      "F1(micro)0.6634615384615384\n",
      "F1(macro)0.6620169622932531\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_test, y_pred, classification=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_nlp]",
   "language": "python",
   "name": "conda-env-deep_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
