{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from models.utils.evaluation import print_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"../features/weebit_train_with_features.csv\", index_col=0)\n",
    "X_test = pd.read_csv(\"../features/weebit_test_with_features.csv\", index_col=0)\n",
    "\n",
    "# get Y\n",
    "y_train = X_train[\"Level\"]\n",
    "y_test = X_test[\"Level\"]\n",
    "\n",
    "# remove Y and Text columns \n",
    "X_train.drop(columns=['Text', 'Level'], inplace=True)\n",
    "X_test.drop(columns=['Text', 'Level'], inplace=True)\n",
    "\n",
    "# whole set; used in cross-validation\n",
    "X = pd.concat([X_train, X_test]).reset_index(drop=True)\n",
    "y = pd.concat([y_train, y_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scoring, we will use __Spearman correlation__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_function = lambda y_true, y_pred: spearmanr(y_true, y_pred)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.random_forest import RandomForest\n",
    "from models.utils.hyperparemeter_optimization import grid_search_cv_for_ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to __find the best hyperparameters.__ We will do this using grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score=0.6783827484395081 | max_depth=5 n_estimators=10\n",
      "score=0.6792103203707903 | max_depth=5 n_estimators=50\n",
      "score=0.6846287855430528 | max_depth=5 n_estimators=100\n",
      "score=0.708638451981671 | max_depth=10 n_estimators=10\n",
      "score=0.7431187239243666 | max_depth=10 n_estimators=50\n",
      "score=0.7347961957166792 | max_depth=10 n_estimators=100\n",
      "score=0.7165482445521892 | max_depth=15 n_estimators=10\n",
      "score=0.7435249110167789 | max_depth=15 n_estimators=50\n",
      "score=0.7435577542109802 | max_depth=15 n_estimators=100\n",
      "score=0.7060044666039577 | max_depth=20 n_estimators=10\n",
      "score=0.7373928691717807 | max_depth=20 n_estimators=50\n",
      "score=0.7424822070044348 | max_depth=20 n_estimators=100\n",
      "\n",
      "Best hyperparemeters are: max_depth=15 n_estimators=100\n"
     ]
    }
   ],
   "source": [
    "# set the hyperparameter grid\n",
    "max_depth_values = [5, 10, 15, 20]\n",
    "n_estimators_values = [10, 50, 100]\n",
    "\n",
    "# perform hyperparameter search\n",
    "max_depth, n_estimators = grid_search_cv_for_ensembles(RandomForest(), max_depth_values, n_estimators_values, X_train, y_train, scoring_function, k=3, verbose=1)\n",
    "\n",
    "print()\n",
    "print(\"Best hyperparemeters are: max_depth=\" + str(max_depth) + \" n_estimators=\" + str(n_estimators))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForest(max_depth=max_depth, n_estimators=n_estimators, save_model=True)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Spearman's correlation coef: 0.776516575205192\n",
      "================================================\n",
      "-----------\n",
      "R^2 = 0.590388561593963\n",
      "R = 0.7683674652104702\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.xgboost import XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost is showing a particular meaningless warning, we will ignore it\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to __find the best hyperparameters.__ We will do this using grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score=0.6419889844170271 | max_depth=5 n_estimators=10\n",
      "score=0.7483377034002218 | max_depth=5 n_estimators=50\n",
      "score=0.756044230026724 | max_depth=5 n_estimators=100\n",
      "score=0.7639983987917685 | max_depth=5 n_estimators=200\n",
      "score=0.6646530489341059 | max_depth=10 n_estimators=10\n",
      "score=0.7148517933037155 | max_depth=10 n_estimators=50\n",
      "score=0.7183191508186861 | max_depth=10 n_estimators=100\n",
      "score=0.7220409705323259 | max_depth=10 n_estimators=200\n",
      "score=0.638643504903206 | max_depth=15 n_estimators=10\n",
      "score=0.7066639837196066 | max_depth=15 n_estimators=50\n",
      "score=0.7075090389794395 | max_depth=15 n_estimators=100\n",
      "score=0.6996964025662665 | max_depth=15 n_estimators=200\n",
      "score=0.6380222543381513 | max_depth=20 n_estimators=10\n",
      "score=0.6924828556627783 | max_depth=20 n_estimators=50\n",
      "score=0.7200795801507662 | max_depth=20 n_estimators=100\n",
      "score=0.6937403812075035 | max_depth=20 n_estimators=200\n",
      "score=0.6403684577690353 | max_depth=30 n_estimators=10\n",
      "score=0.7158550750074734 | max_depth=30 n_estimators=50\n",
      "score=0.6944479228299215 | max_depth=30 n_estimators=100\n",
      "score=0.6871064021407439 | max_depth=30 n_estimators=200\n",
      "\n",
      "Best hyperparemeters are: max_depth=5 n_estimators=200\n"
     ]
    }
   ],
   "source": [
    "## set the hyperparameter grid\n",
    "max_depth_values = [5, 10, 15, 20, 30]\n",
    "n_estimators_values = [10, 50, 100, 200]\n",
    "\n",
    "# perform hyperparameter search\n",
    "max_depth, n_estimators = grid_search_cv_for_ensembles(XGBoost(), max_depth_values, n_estimators_values, X_train, y_train, scoring_function, k=3, verbose=1)\n",
    "\n",
    "print()\n",
    "print(\"Best hyperparemeters are: max_depth=\" + str(max_depth) + \" n_estimators=\" + str(n_estimators))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = XGBoost(save_model=True)\n",
    "\n",
    "xgboost.fit(X_train, y_train)\n",
    "y_pred = xgboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Spearman's correlation coef: 0.7319301576049824\n",
      "================================================\n",
      "-----------\n",
      "R^2 = 0.5230648822711715\n",
      "R = 0.7232322464265345\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find mean Spearman correlation over k folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.support_vector_machine import SupportVectorMachine\n",
    "from models.utils.hyperparemeter_optimization import find_best_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to __find the best hyperparameter C.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score=0.701074722023504 | C=1.0\n",
      "score=0.7183889095741917 | C=2.0\n",
      "score=0.7240213074869728 | C=5.0\n",
      "score=0.7210584680276391 | C=10.0\n",
      "score=0.7248240205447445 | C=20.0\n",
      "\n",
      "Best C is 20.0\n"
     ]
    }
   ],
   "source": [
    "## set the hyperparameter grid\n",
    "c_values = [1.0, 2.0, 5.0, 10.0, 20.0]\n",
    "\n",
    "# perform hyperparameter search\n",
    "best_c = find_best_C(SupportVectorMachine(), c_values, X_train, y_train, scoring_function, k=3, verbose=1)\n",
    "\n",
    "print()\n",
    "print(\"Best C is \" + str(best_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SupportVectorMachine(C=best_c, save_model=True)\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Spearman's correlation coef: 0.7588666565532721\n",
      "================================================\n",
      "-----------\n",
      "R^2 = 0.5478683430743052\n",
      "R = 0.7401812906810772\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from models.multilayer_perceptron import MultilayerPerceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MultilayerPerceptron(input_dim=X_train.shape[1], save_model=True, verbose=0)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Spearman's correlation coef: 0.7852631715901716\n",
      "================================================\n",
      "-----------\n",
      "R^2 = 0.5783411663467266\n",
      "R = 0.7604874531159121\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_nlp]",
   "language": "python",
   "name": "conda-env-deep_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
