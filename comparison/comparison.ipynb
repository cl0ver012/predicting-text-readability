{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical comparison of formulas and models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook a statistical comparison of models is performed.\n",
    "\n",
    "First, go to the parent directory so you can import all modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric used for comparison is Spearman's correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "metric = lambda predA, predB: abs(spearmanr(predA, predB)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical comparison will be performed with the bootstrap significance testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comparison.bootstrap import bootstrap_significance_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of times to perform bootstrap resampling\n",
    "n = int(1e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use __significance level of 0.05.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Comparison of formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_train = pd.read_csv(\"../features/weebit_train_with_features.csv\", index_col=0)\n",
    "X_test = pd.read_csv(\"../features/weebit_test_with_features.csv\", index_col=0)\n",
    "\n",
    "# get Y\n",
    "y_train = X_train[\"Level\"]\n",
    "y_test = X_test[\"Level\"]\n",
    "\n",
    "# remove Y and Text columns \n",
    "X_train.drop(columns=['Text', 'Level'], inplace=True)\n",
    "X_test.drop(columns=['Text', 'Level'], inplace=True)\n",
    "\n",
    "# whole set\n",
    "X = pd.concat([X_train, X_test]).reset_index(drop=True)\n",
    "y = pd.concat([y_train, y_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from formulas.readability_formulas import flesch, dale_chall, gunning_fog\n",
    "\n",
    "X = flesch(X)\n",
    "X = dale_chall(X)\n",
    "X = gunning_fog(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Flesch vs Dale-Chall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35448675874199415"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(y, X[\"Dale_Chall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3600500729354314"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(y, X[\"Flesch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flesch has a slightly higher correlation. But is it statistically significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated p-value: 0.37526\n"
     ]
    }
   ],
   "source": [
    "p_value = bootstrap_significance_testing(y, X['Flesch'], X['Dale_Chall'], metric, n=n)\n",
    "print(\"Estimated p-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the p-value is quite high (>0.05). We fail to reject the null hypothesis: the difference between the Flesch and Dale-Chall formula is not statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Gunning fog vs Flesch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3600500729354314"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(y, X[\"Flesch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43317970094386554"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(y, X[\"Gunning_fog\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gunning fog has a higher correlation. Is this stat. significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated p-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "p_value = bootstrap_significance_testing(y, X['Gunning_fog'], X['Flesch'], metric, n=n)\n",
    "print(\"Estimated p-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is very small (it rounds to 0.0). We can say that Gunning fog formula performs significantlly better than Flesch formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Gunning fog vs Dale-Chall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35448675874199415"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(y, X[\"Dale_Chall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43317970094386554"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(y, X[\"Gunning_fog\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gunning fog has a higher correlation. Is this stat. significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated p-value: 1e-05\n"
     ]
    }
   ],
   "source": [
    "p_value = bootstrap_significance_testing(y, X['Gunning_fog'], X['Dale_Chall'], metric, n=n)\n",
    "print(\"Estimated p-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is very small. We can say that Gunning fog formula performs significantlly better than Dale-Chall formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our tests, there is __no statistical difference between Dale-Chall and Flesch formulas__.\n",
    "\n",
    "__Gunning fog index performs better than both.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparison of machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ML model evaluation (done in `ml_models/model_evaluation.ipynb`), XGBoost and Multilayer Perceptron (MLP) performed the best, with XGBoost performing slightly better than MLP. In this section we will test if the difference between those two models and the rest is statistically significant, and also is XGBoost significantlly better than MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from ml_models.models.random_forest import RandomForest\n",
    "from ml_models.models.xgboost import XGBoost\n",
    "from ml_models.models.support_vector_machine import SupportVectorMachine\n",
    "from ml_models.models.multilayer_perceptron import MultilayerPerceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForest(use_saved_model=True, model_path='../ml_models/models/saved_models/rf.pickle')\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "xgboost = XGBoost(use_saved_model=True, model_path='../ml_models/models/saved_models/xgboost.pickle')\n",
    "y_pred_xgboost = xgboost.predict(X_test)\n",
    "\n",
    "svm = SupportVectorMachine(use_saved_model=True, model_path='../ml_models/models/saved_models/svm.pickle')\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "mlp = MultilayerPerceptron(input_dim=X_train.shape[1], use_saved_model=True, verbose=0, model_path='../ml_models/models/saved_models/mlp.h5')\n",
    "y_pred_mlp = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 MLP vs RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated p-value: 0.12076\n"
     ]
    }
   ],
   "source": [
    "p_value = bootstrap_significance_testing(y_test, y_pred_mlp, y_pred_rf, metric, n=n)\n",
    "print(\"Estimated p-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated p-value is larger than our significance level (0.05). We fail to reject the null hypothesis. __The difference between the MLP and RandomForest models is not statistically significant.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 MLP vs SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated p-value: 0.43394\n"
     ]
    }
   ],
   "source": [
    "p_value = bootstrap_significance_testing(y_test, y_pred_mlp, y_pred_svm, metric, n=n)\n",
    "print(\"Estimated p-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated p-value is larger than our significance level (0.05). We fail to reject the null hypothesis. __The difference between the MLP and SVC models is not statistically significant.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 XGBoost vs RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated p-value: 0.02543\n"
     ]
    }
   ],
   "source": [
    "p_value = bootstrap_significance_testing(y_test, y_pred_xgboost, y_pred_rf, metric, n=n)\n",
    "print(\"Estimated p-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is small (<0.05). __We can say that XGBoost performs significantlly better than the RandomForest model.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 XGBoost vs SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated p-value: 0.30053\n"
     ]
    }
   ],
   "source": [
    "p_value = bootstrap_significance_testing(y_test, y_pred_xgboost, y_pred_svm, metric, n=n)\n",
    "print(\"Estimated p-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated p-value is larger than our significance level (0.05). We fail to reject the null hypothesis. __The difference between the XGBoost and SVC models is not statistically significant.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 XGboost vs MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated p-value: 0.34068\n"
     ]
    }
   ],
   "source": [
    "p_value = bootstrap_significance_testing(y_test, y_pred_xgboost, y_pred_mlp, metric, n=n)\n",
    "print(\"Estimated p-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated p-value is larger than our significance level (0.05). We fail to reject the null hypothesis. __The difference between the MLP and XGBoost models is not statistically significant.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our tests, there is __almost no statistically significant difference between different ML models.__.\n",
    "\n",
    "The only thing which we managed to prove is that __XGBoost outperforms the RandomForest model.__ \n",
    "\n",
    "This gives evidence to the claim that XGBoost is the best ML model we have, althrough the differences are minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison of formulas vs ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the Gunning Fog formula performed the best of the formulas, and XGBoost performed best of the models, we will compare those two. We will try to test if XGBoost model is statistically significantlly better than the Gunning fog formula. \n",
    "\n",
    "The null hypothesis is that there is no difference between XGBoost and the Gunning fog formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = gunning_fog(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated p-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "p_value = bootstrap_significance_testing(y_test, y_pred_xgboost, X_test['Gunning_fog'], metric, n=n)\n",
    "print(\"Estimated p-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is very small (it rounds to 0.0). We reject the null hypothesis, which gives evidence that __XGBoost model is better than the Gunning fog formula.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Our conclusion is that ML models are truly better than traditional formulas.__ Considering they use much more features and are able to learn from them, this comes to no surprise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_nlp]",
   "language": "python",
   "name": "conda-env-deep_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
