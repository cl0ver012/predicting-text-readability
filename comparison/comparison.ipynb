{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical comparison of formulas and models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, a statistical comparison of models is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing path\n",
    "import sys\n",
    "sys.path.insert(0,'..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric used for comparison is Spearman's correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "metric = lambda predA, predB: abs(spearmanr(predA, predB)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical comparison will be performed with the bootstrap significance testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comparison.bootstrap import bootstrap_significance_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of times to perform bootstrap resampling\n",
    "n = int(1e4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use __significance level of 0.05.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Comparison of formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_train = pd.read_csv(\"../features/weebit_train_with_features.csv\", index_col=0)\n",
    "X_test = pd.read_csv(\"../features/weebit_test_with_features.csv\", index_col=0)\n",
    "\n",
    "# get Y\n",
    "y_train = X_train[\"Level\"]\n",
    "y_test = X_test[\"Level\"]\n",
    "\n",
    "# remove Y and Text columns \n",
    "X_train.drop(columns=['Text', 'Level'], inplace=True)\n",
    "X_test.drop(columns=['Text', 'Level'], inplace=True)\n",
    "\n",
    "# whole set\n",
    "X = pd.concat([X_train, X_test]).reset_index(drop=True)\n",
    "y = pd.concat([y_train, y_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from formulas.readability_formulas import flesch, dale_chall, gunning_fog\n",
    "\n",
    "X = flesch(X)\n",
    "X = dale_chall(X)\n",
    "X = gunning_fog(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Flesch vs Dale-Chall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35448675874199415"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(y, X[\"Dale_Chall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3600500729354314"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(y, X[\"Flesch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flesch has a slightly higher correlation. But is it statistically significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated p-value: 0.37526\n"
     ]
    }
   ],
   "source": [
    "p_value = bootstrap_significance_testing(y, X['Flesch'], X['Dale_Chall'], metric, n=n)\n",
    "print(\"Estimated p-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the p-value is quite high (>0.05). We fail to reject the null hypothesis: the difference between the Flesch and Dale-Chall formula is not statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Gunning fog vs Flesch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3600500729354314"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(y, X[\"Flesch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43317970094386554"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(y, X[\"Gunning_fog\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gunning fog has a higher correlation. Is this stat. significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated p-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "p_value = bootstrap_significance_testing(y, X['Gunning_fog'], X['Flesch'], metric, n=n)\n",
    "print(\"Estimated p-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is very small (it rounds to 0.0). We can say that Gunning fog formula performs significantlly better than Flesch formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Gunning fog vs Dale-Chall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35448675874199415"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(y, X[\"Dale_Chall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43317970094386554"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(y, X[\"Gunning_fog\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gunning fog has a higher correlation. Is this stat. significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated p-value: 0.0001\n"
     ]
    }
   ],
   "source": [
    "p_value = bootstrap_significance_testing(y, X['Gunning_fog'], X['Dale_Chall'], metric, n=n)\n",
    "print(\"Estimated p-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is very small. We can say that Gunning fog formula performs significantlly better than Dale-Chall formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our tests, there is __no statistical difference between Dale-Chall and Flesch formulas__.\n",
    "\n",
    "__Gunning fog index performs better than both.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparison of machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ML model evaluation (done in `ml_models/model_evaluation.ipynb`), Random Forest and Multilayer Perceptron (MLP) performed the best, with MLP performing slightly better than Random Forest. In this section we will test if the difference between those two models and the rest is statistically significant, and also is MLP significantlly better than Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from ml_models.models.random_forest import RandomForest\n",
    "from ml_models.models.xgboost import XGBoost\n",
    "from ml_models.models.support_vector_machine import SupportVectorMachine\n",
    "from ml_models.models.multilayer_perceptron import MultilayerPerceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForest(use_saved_model=True, model_path='../ml_models/models/saved_models/rf.pickle')\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "xgboost = XGBoost(use_saved_model=True, model_path='../ml_models/models/saved_models/xgboost.pickle')\n",
    "y_pred_xgboost = xgboost.predict(X_test)\n",
    "\n",
    "svm = SupportVectorMachine(use_saved_model=True, model_path='../ml_models/models/saved_models/svm.pickle')\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "mlp = MultilayerPerceptron(input_dim=X_train.shape[1], use_saved_model=True, verbose=0, model_path='../ml_models/models/saved_models/mlp.h5')\n",
    "y_pred_mlp = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 MLP vs XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated p-value: 0.007\n"
     ]
    }
   ],
   "source": [
    "p_value = bootstrap_significance_testing(y_test, y_pred_mlp, y_pred_xgboost, metric, n=n)\n",
    "print(\"Estimated p-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is small (<0.05). __We can say that MLP performs significantlly better than the XGBoost model.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 MLP vs SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated p-value: 0.084\n"
     ]
    }
   ],
   "source": [
    "p_value = bootstrap_significance_testing(y_test, y_pred_mlp, y_pred_svm, metric, n=n)\n",
    "print(\"Estimated p-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated p-value is larger than our significance level (0.05). We fail to reject the null hypothesis. __The difference between the MLP and SVM models is not statistically significant.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 RandomForest vs XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated p-value: 0.0009\n"
     ]
    }
   ],
   "source": [
    "p_value = bootstrap_significance_testing(y_test, y_pred_rf, y_pred_xgboost, metric, n=n)\n",
    "print(\"Estimated p-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is small (<0.05). __We can say that RandomForest performs significantlly better than the XGBoost model.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 RandomForest vs SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated p-value: 0.1378\n"
     ]
    }
   ],
   "source": [
    "p_value = bootstrap_significance_testing(y_test, y_pred_rf, y_pred_svm, metric, n=n)\n",
    "print(\"Estimated p-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated p-value is larger than our significance level (0.05). We fail to reject the null hypothesis. __The difference between the RandomForest and SVC models is not statistically significant.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 MLP vs Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated p-value: 0.3243\n"
     ]
    }
   ],
   "source": [
    "p_value = bootstrap_significance_testing(y_test, y_pred_mlp, y_pred_rf, metric, n=n)\n",
    "print(\"Estimated p-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated p-value is larger than our significance level (0.05). We fail to reject the null hypothesis. __The difference between the MLP and RandomForest models is not statistically significant.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our tests, there is __almost no statistically significant difference between different ML models.__\n",
    "\n",
    "The only thing which we managed to prove is that __RandomForest and MLP model outperform the XGBoost model.__ \n",
    "\n",
    "This gives evidence to the claim that RandomForest and MLP are the best ML models we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison of formulas vs ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the Gunning Fog formula performed the best of the formulas, and MLP (along with RandomForest) performed best of the models, we will compare those two. We will try to test if MLP model is statistically significantlly better than the Gunning fog formula. \n",
    "\n",
    "The null hypothesis is that there is no difference between MLP model and the Gunning fog formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = gunning_fog(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated p-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "p_value = bootstrap_significance_testing(y_test, y_pred_mlp, X_test['Gunning_fog'], metric, n=n)\n",
    "print(\"Estimated p-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is very small (it rounds to 0.0). We reject the null hypothesis, which gives evidence that __the MLP model is better than the Gunning fog formula.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Our conclusion is that ML models are truly better than traditional formulas.__ Considering they use much more features and are able to learn from them, this comes to no surprise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_nlp]",
   "language": "python",
   "name": "conda-env-deep_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
