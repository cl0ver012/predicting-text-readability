{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical comparison of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook a statistical comparison of models is performed.\n",
    "\n",
    "First, go to the parent directory so you can import all modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric used for comparison is Spearman's correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "metric = lambda predA, predB: abs(spearmanr(predA, predB)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical comparison will be performed with the bootstrap significance testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comparison.bootstrap import bootstrap_significance_testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Comparison of formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_train = pd.read_csv(\"../features/weebit_train_with_features.csv\", index_col=0)\n",
    "X_test = pd.read_csv(\"../features/weebit_test_with_features.csv\", index_col=0)\n",
    "\n",
    "# get Y\n",
    "y_train = X_train[\"Level\"]\n",
    "y_test = X_test[\"Level\"]\n",
    "\n",
    "# remove Y and Text columns \n",
    "X_train.drop(columns=['Text', 'Level'], inplace=True)\n",
    "X_test.drop(columns=['Text', 'Level'], inplace=True)\n",
    "\n",
    "# whole set\n",
    "X = pd.concat([X_train, X_test]).reset_index(drop=True)\n",
    "y = pd.concat([y_train, y_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from formulas.readability_formulas import flesch, dale_chall, gunning_fog\n",
    "\n",
    "X = flesch(X)\n",
    "X = dale_chall(X)\n",
    "X = gunning_fog(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Compare Flesch vs Dale-Chall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.384"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap_significance_testing(y, X['Flesch'], X['Dale_Chall'], metric, n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap_significance_testing(y, X['Gunning_fog'], X['Flesch'], metric, n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_nlp]",
   "language": "python",
   "name": "conda-env-deep_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
